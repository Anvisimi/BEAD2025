{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anvisimi/BEAD2025/blob/main/colab/03_Apache_Spark_Core_Functions_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Apache PySpark\n",
        "In this demo we will see how we can run PySpark in a Google\n",
        " Colaboratory notebook. We will also perform some basic data exploratory tasks common to data science problems.\n",
        "\n"
      ],
      "metadata": {
        "id": "jaVSpe-PbxkO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PySpark Install\n",
        "\n",
        "The first step involves installing pyspark.  The next step is to install findspark library.\n",
        "\n",
        "*Note: the --ignore-install flag is used to ignore previous installations and use the latest one built alongside the allocated cluster.*\n"
      ],
      "metadata": {
        "id": "26ugKWTTdmm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install pyspark using pip\n",
        "!pip install --ignore-install -q pyspark\n",
        "# install findspark using pip\n",
        "!pip install --ignore-install -q findspark"
      ],
      "metadata": {
        "id": "7cdc0oJjb8N5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86f1a34e-1928-4c86-cbda-c3c33667b2f5"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spark Session\n",
        "\n",
        "We import the basic object SparkSession from the Spark Framework. In PySpark, a Spark Session is a unified entry point for reading data, configuring the system, and managing various Spark services.\n",
        "\n",
        "Here's a breakdown of what the Spark Session does:\n",
        "\n",
        "1. Unified Entry Point: It's the central point to access all Spark  functionalities, making it simpler and more intuitive to use Spark for development.\n",
        "2. Data Reading and Writing: We use the Spark Session to read data from various sources (like HDFS, S3, JDBC, Hive, etc.) and write data to various sinks.\n",
        "3. Configuration Management: It allows us to configure various aspects of the Spark application, such as setting configuration parameters.\n",
        "4. Creating DataFrames and Datasets: The Spark Session provides methods to create DataFrames and Datasets, which are the core data structures in Spark.\n",
        "5. Execution of SQL Queries: We can run SQL queries by using the Spark Session, especially when dealing with structured data.\n",
        "6. Managing Spark Services: It also helps in managing underlying Spark services like SparkContext, and it's the main point of interaction when dealing with structured data.\n",
        "\n",
        "In PySpark, a Spark Session is created using the SparkSession.builder method. Here's an example:"
      ],
      "metadata": {
        "id": "c7coAqaRcsJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "# import collections\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"My App \").getOrCreate()"
      ],
      "metadata": {
        "id": "l1I1VynS4JBT"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sparkContext"
      ],
      "metadata": {
        "id": "c-XvTqWhANe5",
        "outputId": "b08777b5-daad-4e35-adb4-1da72bea494d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SparkContext master=local appName=My App >"
            ],
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://cae9914ebdff:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.4</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>My App </code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Count\n",
        "To count the number of words from a file."
      ],
      "metadata": {
        "id": "ey77Nt4npScI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wc = spark.sparkContext.textFile(\"demo.txt\") \\\n",
        "   .flatMap(lambda line: line.split(\" \")) \\\n",
        "   .map(lambda word: (word, 1)) \\\n",
        "    .reduceByKey(lambda a, b: a + b)\n",
        "print(wc.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5SLOVbZpVju",
        "outputId": "c19c8c85-4aad-4d20-bebe-ca643b9ddeed"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('apache', 3), ('kafka', 1), ('spark', 1), ('tomcat', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EQ7zyons1mo",
        "outputId": "6652d2c6-1821-4e4e-a9bb-4832289ab651"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Line Count\n",
        "To count the number of lines from a file."
      ],
      "metadata": {
        "id": "Ux4bAHTnv70Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "someFile = \"wordcount.txt\"\n",
        "# the above file is under your pythonProject folder\n",
        "spark = SparkSession.builder.appName(\"SimpleApp\").getOrCreate()\n",
        "print(spark.read.text(someFile).count())\n"
      ],
      "metadata": {
        "id": "wRm36sJXv7mr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30220a54-f828-4c68-86f7-04bf122d9a5c"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mounting Google Drive\n",
        "Connect to Google Drive"
      ],
      "metadata": {
        "id": "AVsBhvHTvokF"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddxRp3RNh1Fx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caf3d953-0a9a-46a1-8932-d6e3c6dfa946"
      },
      "source": [
        "# to read in data from a text file, first upload the data file into your google drive and then mount your google drive onto colab\n",
        "from google.colab import drive\n",
        "# to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True)\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "someFile = \"/content/drive/MyDrive/data/Customer.csv\"\n",
        "# the above file is under your pythonProject folder\n",
        "spark = SparkSession.builder.appName(\"SimpleApp\").getOrCreate()\n",
        "print(spark.read.text(someFile).count())\n"
      ],
      "metadata": {
        "id": "9aPoi-6RvrFe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5418376-d6cd-4a32-d755-84d917bbb7c1"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "51\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Creation\n",
        "\n",
        "### Create a simple from spark RDD\n",
        "In this examples below we want to see how to create a simple data structure using spark core commands\n",
        "\n",
        "#### Example 1: From RDD\n",
        "To create an RDD using a SparkSession in PySpark, you first need to initialize a SparkSession and then use it to create an RDD. Here's a simple example where we'll create an RDD from a tuple of numbers using a SparkSession"
      ],
      "metadata": {
        "id": "dJWVmhPJdK6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an RDD from a list of numbers\n",
        "numbers = (1, 2, 3, 4, 5)\n",
        "numbers_rdd = spark.sparkContext.parallelize(numbers)\n",
        "# Syntax print(spark.sparkContext.parallelize(\"(A B C)\").collect())\n",
        "print(f\"using collect() function\",numbers_rdd.collect())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TW2Dw06WllqY",
        "outputId": "55b0b72b-d115-4c55-81b7-3d7d430cc84f"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using collect() function [1, 2, 3, 4, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Alternative is to convert to a new data structure that is print friendly\n",
        "print(f\"using pipeline of functions\")\n",
        "print(tuple(spark.sparkContext.parallelize(numbers).collect()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpBiYXVYFTeR",
        "outputId": "20ab2b01-f299-40e6-9930-983b0e4d74aa"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using pipeline of functions\n",
            "(1, 2, 3, 4, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outfile = spark.read.text(\"/content/drive/MyDrive/data/guttenberg/AliceAdventuresInWonderland.txt\")"
      ],
      "metadata": {
        "id": "7jZjqGDPF6LM"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let us count the lines in the out.txt"
      ],
      "metadata": {
        "id": "R7FUs4kZGTIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(outfile.count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZU5TmvCGWk6",
        "outputId": "e60f70e5-2c0e-478c-a9cc-d168be55a7fd"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3760\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example 2: From Local File\n",
        "\n",
        "To create an RDD in PySpark by reading data from a CSV file, such as \"customers.csv\", you'll use a SparkSession to read the CSV and then convert the DataFrame to an RDD. Here's a step-by-step example:\n",
        "\n",
        "1. Initialize a SparkSession. (Done)\n",
        "2. Read the \"customers.csv\" file into a DataFrame.\n",
        "3. Convert the DataFrame to an RDD.\n",
        "4. Perform a simple action on the RDD, like counting the number of records.\n",
        "\n",
        "Here's the code snippet for this process:"
      ],
      "metadata": {
        "id": "mguhdx0po7H0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customers = spark.read.csv(\"/content/drive/MyDrive/data/Customer.csv\", header=True, inferSchema=True).rdd\n",
        "\n",
        "# Perform a simple action: count the number of records\n",
        "record_count = customers.count()\n",
        "print(f\"Number of records: {record_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmFLTKiupPZ-",
        "outputId": "cb27b0d1-b596-4a34-8a29-9797dd744a97"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of records: 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example 2: From Local TEXT File\n",
        "\n",
        "To create an RDD in PySpark by reading data from a CSV file, such as \"customers.csv\", you'll use a SparkSession to read the CSV and then convert the DataFrame to an RDD. Here's a step-by-step example:\n",
        "\n",
        "1. Initialize a SparkSession. (Done)\n",
        "2. Read the \"airports.txt\" file into a DataFrame.\n",
        "3. Convert the DataFrame to an RDD.\n",
        "4. Perform a simple actions on this RDD latere.\n",
        "\n",
        "A text dataset is pointed to by path. The path can be either a single text file or a directory of text files."
      ],
      "metadata": {
        "id": "Vxpc9XjmFrfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "airports = spark.read.text(\"/content/drive/MyDrive/data/airport-data/airports.text\")"
      ],
      "metadata": {
        "id": "c1bAF3KFF7OY"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Actions and Transformation\n",
        "\n",
        "In PySpark, operations on RDDs can be broadly classified into two categories: transformations and actions. Transformations create a new RDD from an existing one, while actions return a value after running a computation on the RDD. Below are simple examples demonstrating the use of transformations and actions.\n",
        "\n",
        "###Transformations\n",
        "\n",
        "####Map\n",
        "Applies a function to each element and returns a new RDD.\n"
      ],
      "metadata": {
        "id": "EL3V5wUNridx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = spark.sparkContext.parallelize((1, 2, 3, 4, 5))\n",
        "# Traditional Python map(function, collection) (few MBs - GB fails)\n",
        "# Scalabale map (Peta - support)\n",
        "# iterablecollection.map(function) -> Object\n",
        "# collect() Object to collection\n",
        "print(tuple(rdd.map(lambda x: x * x).collect()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3o9yPBOr-ss",
        "outputId": "53d27d58-b612-4ff8-8fc0-40b071a3fbbe"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 4, 9, 16, 25)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Filter\n",
        "Returns a new RDD containing only the elements that satisfy a condition."
      ],
      "metadata": {
        "id": "ytxb4hDSsMkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(rdd.filter(lambda x: x % 2 == 0).collect())  # Keeps even numbers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJKFicO-sSrR",
        "outputId": "b3bbc75f-6e4c-4fa9-a153-fd8c4a765388"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####FlatMap\n",
        "Similar to map, but each input item can be mapped to 0 or more output items."
      ],
      "metadata": {
        "id": "7hK0FzEUsZQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = spark.sparkContext.parallelize([\"hello world\", \"hi\", \"hello mars\", \"hello jupiter\", \"hello saturn\"])\n",
        "print(words.flatMap(lambda x: x.split(\" \")).collect())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BrAp7Uwskim",
        "outputId": "19d84109-e2b0-4f6f-f7e7-4c577a5e8744"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', 'world', 'hi', 'hello', 'mars', 'hello', 'jupiter', 'hello', 'saturn']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Distinct\n",
        "Returns a new RDD containing distinct elements from the original RDD."
      ],
      "metadata": {
        "id": "eDWTyCeks1iE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(words.flatMap(lambda x: x.split(\" \")).distinct().collect())"
      ],
      "metadata": {
        "id": "74xIHlGchlEE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56ac4a91-27f2-4b34-81bf-2eabfd90f330"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', 'world', 'hi', 'mars', 'jupiter', 'saturn']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tuples = spark.sparkContext.parallelize((1, 1, 2, 3, 3, 4))\n",
        "print(tuples.distinct().collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGk2e8kLs6cr",
        "outputId": "19e3945e-3926-4994-c0fc-1f4729749c02"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EV27CJOfIJxv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Actions\n",
        "####Collect\n",
        "Returns all the elements of the RDD as an array to the driver program."
      ],
      "metadata": {
        "id": "uO6zm2ZgtPYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tuples.distinct().collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrJG5bFCtaIy",
        "outputId": "76a259a3-dfbd-41cf-a638-3bd7483092c2"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Count\n",
        "Returns the number of elements in the RDD."
      ],
      "metadata": {
        "id": "w9B3hqYsth7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tuples.count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yi1FyZRtlcn",
        "outputId": "1c78e2c7-afc4-4d58-e39d-564b5d4b7287"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Take\n",
        "Returns an array with the first n elements of the RDD."
      ],
      "metadata": {
        "id": "R6AhdGsltqht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_three = tuples.take(3)\n",
        "print(first_three)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_EERBI9tuQu",
        "outputId": "86b750da-d615-444d-b00c-a0d392c7270a"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "customers.take(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q56QYaL7N4OH",
        "outputId": "190052de-bebf-43fe-b903-1835ccd3401a"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(CustomerID=1000, CustomerName='Lou Anna Tan', MemberCategory='A', Age=29, Gender='F', AmountSpent=4.14, Address='Blk 26, Telok Blangah Crescent #22-87, Singapore 0409', City='Frankfurt', CountryCode='GER', ContactTitle='Ms', PhoneNumber=2732287),\n",
              " Row(CustomerID=1001, CustomerName='Wong Sook Huey', MemberCategory='A', Age=37, Gender='F', AmountSpent=67.1, Address='Blk 1007 Teresa Ville Lower Delta Road #06-02, Singapore 0410', City='Singapore', CountryCode='SIN', ContactTitle='Ms', PhoneNumber=2740975)]"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Reduce\n",
        "Aggregates the elements of the RDD using a function."
      ],
      "metadata": {
        "id": "Zk6Xizictyfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sum1 = tuples.reduce(lambda a, b: a + b)\n",
        "print(sum1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IY7SkGfvt2-g",
        "outputId": "1f9bbb81-7980-425c-b27c-6b291f4aa9d9"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These examples illustrate basic operations in PySpark, allowing you to manipulate and analyze large datasets efficiently. To run these examples, ensure you have a SparkContext (sc) initialized in your PySpark environment.\n",
        "\n",
        "### How to pretty print in PySaprk?\n",
        "\n",
        "The take() function and iteration in PySpark will mimic the pretty print function, but use them wisely."
      ],
      "metadata": {
        "id": "fX1Ur6gyqFDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pprint()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5y1ezBHTOrAf",
        "outputId": "b9afc7f0-b7a3-465d-aa1c-eab21e348511"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretty printing has been turned OFF\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"First five records of customer data set\", customers.take(5))\n",
        "print(\"Not so pretty....\")\n",
        "print(\"Now let us pretty print:\")\n",
        "# To pretty print, you need to iterate\n",
        "for element in customers.take(10):\n",
        "    print(element)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QcNCFRZqkiL",
        "outputId": "e7a6b6c6-3929-4653-d012-5bf985decbc0"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First five records of customer data set [Row(CustomerID=1000, CustomerName='Lou Anna Tan', MemberCategory='A', Age=29, Gender='F', AmountSpent=4.14, Address='Blk 26, Telok Blangah Crescent #22-87, Singapore 0409', City='Frankfurt', CountryCode='GER', ContactTitle='Ms', PhoneNumber=2732287), Row(CustomerID=1001, CustomerName='Wong Sook Huey', MemberCategory='A', Age=37, Gender='F', AmountSpent=67.1, Address='Blk 1007 Teresa Ville Lower Delta Road #06-02, Singapore 0410', City='Singapore', CountryCode='SIN', ContactTitle='Ms', PhoneNumber=2740975), Row(CustomerID=1002, CustomerName='Ng Choon Seng', MemberCategory='C', Age=23, Gender='M', AmountSpent=63.18, Address='Blk 63 Bishan St 21 #06-01, Singapore 1057', City='Toronto', CountryCode='CAN', ContactTitle='Mr', PhoneNumber=2580742), Row(CustomerID=1003, CustomerName='Chew Teck Kuan', MemberCategory='C', Age=63, Gender='M', AmountSpent=64.49, Address='Blk 109 Bedok North Rd #06-2316, Singapore 1046', City='Singapore', CountryCode='SIN', ContactTitle='Mr', PhoneNumber=4434675), Row(CustomerID=1111, CustomerName='Steven Ou', MemberCategory='B', Age=61, Gender='M', AmountSpent=44.51, Address='Blk 244, Bukit Panjang Ring Road, #11-184, Singapore 2367', City='Rio ', CountryCode='BRA', ContactTitle='Mr', PhoneNumber=7620324)]\n",
            "Not so pretty....\n",
            "Now let us pretty print:\n",
            "Row(CustomerID=1000, CustomerName='Lou Anna Tan', MemberCategory='A', Age=29, Gender='F', AmountSpent=4.14, Address='Blk 26, Telok Blangah Crescent #22-87, Singapore 0409', City='Frankfurt', CountryCode='GER', ContactTitle='Ms', PhoneNumber=2732287)\n",
            "Row(CustomerID=1001, CustomerName='Wong Sook Huey', MemberCategory='A', Age=37, Gender='F', AmountSpent=67.1, Address='Blk 1007 Teresa Ville Lower Delta Road #06-02, Singapore 0410', City='Singapore', CountryCode='SIN', ContactTitle='Ms', PhoneNumber=2740975)\n",
            "Row(CustomerID=1002, CustomerName='Ng Choon Seng', MemberCategory='C', Age=23, Gender='M', AmountSpent=63.18, Address='Blk 63 Bishan St 21 #06-01, Singapore 1057', City='Toronto', CountryCode='CAN', ContactTitle='Mr', PhoneNumber=2580742)\n",
            "Row(CustomerID=1003, CustomerName='Chew Teck Kuan', MemberCategory='C', Age=63, Gender='M', AmountSpent=64.49, Address='Blk 109 Bedok North Rd #06-2316, Singapore 1046', City='Singapore', CountryCode='SIN', ContactTitle='Mr', PhoneNumber=4434675)\n",
            "Row(CustomerID=1111, CustomerName='Steven Ou', MemberCategory='B', Age=61, Gender='M', AmountSpent=44.51, Address='Blk 244, Bukit Panjang Ring Road, #11-184, Singapore 2367', City='Rio ', CountryCode='BRA', ContactTitle='Mr', PhoneNumber=7620324)\n",
            "Row(CustomerID=1634, CustomerName='Sridharan Jayanthi', MemberCategory='A', Age=55, Gender='F', AmountSpent=61.51, Address='Blk 232, Jurong East Street 21 #02-436, Singapore 1234', City='Singapore', CountryCode='SIN', ContactTitle='Ms', PhoneNumber=6658037)\n",
            "Row(CustomerID=1681, CustomerName='Terence Lim', MemberCategory='C', Age=30, Gender='M', AmountSpent=59.62, Address='Blk 99, Balestier Road, #12-168, Singapore 1232', City='Columbo', CountryCode='SLR', ContactTitle='Mr', PhoneNumber=3551385)\n",
            "Row(CustomerID=1810, CustomerName='Vanessa Ong', MemberCategory='C', Age=32, Gender='F', AmountSpent=80.97, Address='Blk 20, Eunos Crescent, #04-2965, Singapore 1400', City='Singapore', CountryCode='SIN', ContactTitle='Ms', PhoneNumber=7487923)\n",
            "Row(CustomerID=1811, CustomerName='Koh Ting Ting', MemberCategory='B', Age=57, Gender='F', AmountSpent=21.52, Address='Blk 61, Upper Paya Lebar Road, Singapore 1953', City='Singapore', CountryCode='SIN', ContactTitle='Ms', PhoneNumber=2827208)\n",
            "Row(CustomerID=1818, CustomerName='Chionh Choon Lee', MemberCategory='A', Age=57, Gender='M', AmountSpent=7.13, Address='Blk 89, Zion Road, #16-137, Singapore 0316', City='Singapore', CountryCode='SIN', ContactTitle='Mr', PhoneNumber=7333100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Key Operations\n",
        "\n",
        "PySpark examples for key based functions are groupByKey, reduceByKey, and sortByKey operations. Let us look at how they work.\n",
        "\n",
        "####groupByKey\n",
        "This operation groups the values for each key in the RDD into a single sequence.\n",
        "####reduceByKey\n",
        "This operation merges the values for each key using an associative reduce function.\n",
        "####sortByKey\n",
        "This operation sorts the dataset by keys.\n",
        "\n",
        "Let us put together an example to compare and contrast\n"
      ],
      "metadata": {
        "id": "S8HxXONGuoxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = spark.sparkContext.parallelize([(3, 6),(1, 2),(3, 4)])\n",
        "grouped = rdd.groupByKey()\n",
        "for key, values in grouped.collect():\n",
        "    print(f\"{key}: {tuple(values)}\")\n",
        "reduced = rdd.reduceByKey(lambda a, b: a + b)\n",
        "print(reduced.collect())\n",
        "sorted_rdd = rdd.sortByKey()\n",
        "print(sorted_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cf_-cHMhvD0V",
        "outputId": "89289648-1e97-4958-8d08-1122c1e705a2"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3: (6, 4)\n",
            "1: (2,)\n",
            "[(3, 10), (1, 2)]\n",
            "[(1, 2), (3, 6), (3, 4)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please note that these operations are transformations and require an action like collect to retrieve the data. Also, keep in mind that groupByKey can cause a lot of data shuffling over the network, and it's generally more efficient to use reduceByKey where possible because it combines output values locally before sending data over the network."
      ],
      "metadata": {
        "id": "SETKMpJDwAZK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sampling\n",
        "\n",
        " The sample() transformation is used to sample a fraction of the data from an RDD. You can sample with or without replacement. Here's how you can use it:\n",
        "\n",
        "####Sampling without replacement\n"
      ],
      "metadata": {
        "id": "3-XSy6Lqwt4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, use the sample function to take a random sample of about 10% of the customers without replacement\n",
        "sampled_customers_rdd = customers.sample(False, 0.2)\n",
        "\n",
        "# Collect the results\n",
        "sampled_customers = sampled_customers_rdd.collect()\n",
        "\n",
        "# Print the sampled list of customers\n",
        "for customer in sampled_customers:\n",
        "    print(customer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rfRqPNOw3Ft",
        "outputId": "aec53d7c-6997-41ee-87d7-369abf058f2c"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row(CustomerID=1002, CustomerName='Ng Choon Seng', MemberCategory='C', Age=23, Gender='M', AmountSpent=63.18, Address='Blk 63 Bishan St 21 #06-01, Singapore 1057', City='Toronto', CountryCode='CAN', ContactTitle='Mr', PhoneNumber=2580742)\n",
            "Row(CustomerID=2345, CustomerName='Ng Teck Kie Anthony', MemberCategory='A', Age=56, Gender='M', AmountSpent=73.93, Address='Blk 105, Gangsa Road, #02-103, Singapore 2367', City='Singapore', CountryCode='SIN', ContactTitle='Mr', PhoneNumber=7690237)\n",
            "Row(CustomerID=2688, CustomerName='Kathleen Loh Swat Hong', MemberCategory='A', Age=38, Gender='F', AmountSpent=39.16, Address='Blk 56, #08-161 Telok Blangah Heights, Singapore 0410', City='Singapore', CountryCode='SIN', ContactTitle='Ms', PhoneNumber=2735765)\n",
            "Row(CustomerID=2741, CustomerName='Goh Chee Eng', MemberCategory='C', Age=45, Gender='F', AmountSpent=25.91, Address='Blk 267 Sembawang Drive #08-349 Singapore 2369', City='Singapore', CountryCode='SIN', ContactTitle='Ms', PhoneNumber=5553849)\n",
            "Row(CustomerID=2876, CustomerName='Kan May Yoke Michelle', MemberCategory='C', Age=43, Gender='F', AmountSpent=56.61, Address='Blk 710 Ang Mo Kio Ave 8 #04-2613, Singapore 2056', City='Singapore', CountryCode='SIN', ContactTitle='Ms', PhoneNumber=4554591)\n",
            "Row(CustomerID=4321, CustomerName='Kok Kah Chee', MemberCategory='C', Age=51, Gender='M', AmountSpent=99.52, Address='Blk 224, Jurong North St 2, #100-01, Singapore 2264', City='Penang', CountryCode='MAL', ContactTitle='Mr', PhoneNumber=5696231)\n",
            "Row(CustomerID=5156, CustomerName='Lee Boon Kiat', MemberCategory='A', Age=32, Gender='M', AmountSpent=73.91, Address='26A, Lengkong Satu, Singapore 1441', City='Singapore', CountryCode='SIN', ContactTitle='Mr', PhoneNumber=4423368)\n",
            "Row(CustomerID=5168, CustomerName='Constance Yong', MemberCategory='C', Age=46, Gender='F', AmountSpent=30.64, Address='Blk 673 Bukit Batok St 67 #08-987 Singapore 1968', City='Singapore', CountryCode='SIN', ContactTitle='Ms', PhoneNumber=7771357)\n",
            "Row(CustomerID=7616, CustomerName='Cheong Pei Sian', MemberCategory='B', Age=57, Gender='F', AmountSpent=92.57, Address='8 Westwood Walk, Singapore 1234', City='Singapore', CountryCode='SIN', ContactTitle='Ms', PhoneNumber=7918584)\n",
            "Row(CustomerID=8888, CustomerName='Kelvin Koh', MemberCategory='B', Age=53, Gender='M', AmountSpent=16.0, Address='Blk 25, Towner Road, #04-09, Singapore 2345', City='Singapore', CountryCode='SIN', ContactTitle='Mr', PhoneNumber=7789876)\n",
            "Row(CustomerID=8929, CustomerName='Neo Keng Hoe', MemberCategory='B', Age=25, Gender='M', AmountSpent=83.44, Address='Blk 492 Yishun Ave 4 #18-168 Singapore 2776', City='Singapore', CountryCode='SIN', ContactTitle='Mr', PhoneNumber=7568912)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the sample method:\n",
        "\n",
        "1. The first argument is withReplacement. Set it to False for sampling without replacement, meaning a particular customer can be chosen only once.\n",
        "2. The second argument is the fraction of the data to sample, which is 0.1 in this case, meaning approximately 10% of the data.\n",
        "\n",
        "This will output a random sample of the customers from your customers_rdd. The collect() action is used here for demonstration purposes, and it should be used with caution if the dataset is large, as it will gather all the sampled data to the driver node.\n",
        "\n",
        "####Sampling with replacement\n",
        "\n",
        "The following example shows how to use sample() with replacement. This means an element can be included in the sample multiple times."
      ],
      "metadata": {
        "id": "TZYw0oFExAcm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, use the sample function to take a random sample of about 10% of the customers with replacement\n",
        "sampled_customers_rdd = customers.sample(True, 0.2)\n",
        "\n",
        "# Collect the results\n",
        "sampled_customers = sampled_customers_rdd.collect()\n",
        "\n",
        "# Print the sampled list of customers\n",
        "for customer in sampled_customers:\n",
        "    print(customer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzQm9EbUxEq4",
        "outputId": "dac782fd-b950-45b4-fbdf-e099088eb9a9"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row(CustomerID=1634, CustomerName='Sridharan Jayanthi', MemberCategory='A', Age=55, Gender='F', AmountSpent=61.51, Address='Blk 232, Jurong East Street 21 #02-436, Singapore 1234', City='Singapore', CountryCode='SIN', ContactTitle='Ms', PhoneNumber=6658037)\n",
            "Row(CustomerID=2323, CustomerName='Richard Kwan', MemberCategory='A', Age=26, Gender='M', AmountSpent=89.52, Address='Blk 27, Marine Crescent, #05-05, Singapore 2345', City='Singapore', CountryCode='SIN', ContactTitle='Mr', PhoneNumber=2352345)\n",
            "Row(CustomerID=7345, CustomerName='P Ravichandran', MemberCategory='B', Age=34, Gender='M', AmountSpent=78.73, Address='Blk 612, Clementi West Street 1, #09-290, Singapore 2612', City='Singapore', CountryCode='SIN', ContactTitle='Mr', PhoneNumber=7755113)\n",
            "Row(CustomerID=8080, CustomerName='Chan Chin Fung', MemberCategory='B', Age=56, Gender='M', AmountSpent=24.95, Address='6 Dover Rise, #17-11, Singapore 1234', City='Singapore', CountryCode='SIN', ContactTitle='Mr', PhoneNumber=8738529)\n",
            "Row(CustomerID=9999, CustomerName='Seah Yang Hwee Raymond', MemberCategory='A', Age=61, Gender='M', AmountSpent=67.18, Address='Blk 410, Sembawang Drive, #14-768, Singapore 1675', City='Singapore', CountryCode='SIN', ContactTitle='Mr', PhoneNumber=5551082)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These examples will give you an array of customers sampled from the original RDD. The actual elements in the sample will vary each time you run the code due to the randomness of the sampling process."
      ],
      "metadata": {
        "id": "nJGdC0dfxpg4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### More Transformations\n",
        "In PySpark, you can perform various RDD operations such as union, join, and cartesian (cross) to combine data in different ways. Here are simple examples for each:\n",
        "#### Union\n",
        "The union operation combines two RDDs to form a new RDD that contains elements from both RDDs."
      ],
      "metadata": {
        "id": "QYMoA6iZx7kL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create two RDDs\n",
        "rdd1 = spark.sparkContext.parallelize([(\"Alice\", 1), (\"Bob\", 2)])\n",
        "rdd2 = spark.sparkContext.parallelize([(\"Charlie\", 3), (\"David\", 4)])\n",
        "\n",
        "# Perform the union operation\n",
        "union_rdd = rdd1.union(rdd2)\n",
        "\n",
        "# Collect and print the results\n",
        "print(union_rdd.collect())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hr-ClfANyEsJ",
        "outputId": "cc316694-8789-4a80-e586-9eaf67d2a343"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Alice', 1), ('Bob', 2), ('Charlie', 3), ('David', 4)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Join\n",
        "The join operation combines two RDDs based on their key."
      ],
      "metadata": {
        "id": "4V_PebbQyQ2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create two RDDs with common keys\n",
        "rdd3 = spark.sparkContext.parallelize([(\"Alice\", \"Apple\"), (\"Bob\", \"Banana\")])\n",
        "rdd4 = spark.sparkContext.parallelize([(\"Alice\", 1), (\"Bob\", 2)])\n",
        "\n",
        "# Perform the join operation\n",
        "join_rdd = rdd3.join(rdd4)\n",
        "\n",
        "# Collect and print the results\n",
        "print(join_rdd.collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsgtMDGHyVK6",
        "outputId": "c5be32b5-87ed-4462-9502-f56937d9cd51"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Alice', ('Apple', 1)), ('Bob', ('Banana', 2))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Cross or Catesian\n",
        "The cartesian operation returns all possible pairs of (a, b) where a is in the first RDD and b is in the second RDD."
      ],
      "metadata": {
        "id": "kwcN4mg6ykED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create two RDDs\n",
        "rdd5 = spark.sparkContext.parallelize([1, 2])\n",
        "rdd6 = spark.sparkContext.parallelize([\"a\", \"b\"])\n",
        "\n",
        "# Perform the cartesian operation\n",
        "cross_rdd = rdd5.cartesian(rdd6)\n",
        "\n",
        "# Collect and print the results\n",
        "print(cross_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nw643DrQypKe",
        "outputId": "cefaae34-d5a7-4a3d-e3a0-aa3666294979"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(1, 'a'), (1, 'b'), (2, 'a'), (2, 'b')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please note that the cartesian operation can be very expensive in terms of computation and memory usage, especially with large datasets, because it forms all possible combinations of elements between the two RDDs.\n",
        "\n",
        "#### mapValues\n",
        "mapValues transformation is applied to RDD datasets that consist of key-value pairs, and it allows us to transform the value of each pair while keeping the key unchanged. Here's a simple example of how we can use mapValues in PySpark:\n",
        "\n"
      ],
      "metadata": {
        "id": "0cMQA5AkywVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a Pair RDD with student ID as the key and a list of grades as the value\n",
        "studentdata = [(1, [88, 92, 96]), (2, [78, 81, 85]), (3, [68, 72, 74])]\n",
        "studentrdd = spark.sparkContext.parallelize(studentdata)\n",
        "\n",
        "# Function to calculate average\n",
        "def calculate_average(grades):\n",
        "    return sum(grades) / len(grades)\n",
        "\n",
        "# Using mapValues to apply the calculate_average function to each value\n",
        "average_grades = studentrdd.mapValues(calculate_average).toDF()\n",
        "\n",
        "\n",
        "# Print the results\n",
        "for result in average_grades:\n",
        "    print(f\"Student ID: {result[0]}, Average Grade: {result[1]:.2f}\")\n"
      ],
      "metadata": {
        "id": "2NxxdXudJhEY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7b25c224-51c8-4b35-ae9b-e71703611c3e"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 93.0 failed 1 times, most recent failure: Lost task 0.0 in stage 93.0 (TID 96) (cae9914ebdff executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/local/lib/python3.11/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/lib/python3.11/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/pyspark/rdd.py\", line 2849, in takeUpToNumLeft\n    yield next(iterator)\n          ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/pyspark/rdd.py\", line 4260, in map_values_fn\n    return kv[0], f(kv[1])\n                  ^^^^^^^^\n  File \"<ipython-input-85-271fa4f254c1>\", line 7, in calculate_average\nTypeError: 'int' object is not callable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat jdk.internal.reflect.GeneratedMethodAccessor102.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/local/lib/python3.11/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/lib/python3.11/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/pyspark/rdd.py\", line 2849, in takeUpToNumLeft\n    yield next(iterator)\n          ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/pyspark/rdd.py\", line 4260, in map_values_fn\n    return kv[0], f(kv[1])\n                  ^^^^^^^^\n  File \"<ipython-input-85-271fa4f254c1>\", line 7, in calculate_average\nTypeError: 'int' object is not callable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-271fa4f254c1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Using mapValues to apply the calculate_average function to each value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0maverage_grades\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudentrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalculate_average\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mtoDF\u001b[0;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \"\"\"\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampleRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoDF\u001b[0m  \u001b[0;31m# type: ignore[assignment]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1441\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m             )\n\u001b[0;32m-> 1443\u001b[0;31m         return self._create_dataframe(\n\u001b[0m\u001b[1;32m   1444\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1483\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1484\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m   1054\u001b[0m         \"\"\"\n\u001b[1;32m   1055\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0mtupled_rdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchema\u001b[0;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[1;32m    994\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStructType\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m         \"\"\"\n\u001b[0;32m--> 996\u001b[0;31m         \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    997\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSized\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m             raise PySparkValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2886\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2887\u001b[0m         \"\"\"\n\u001b[0;32m-> 2888\u001b[0;31m         \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2889\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2890\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   2853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2855\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2856\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2857\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   2508\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2509\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2510\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2511\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 93.0 failed 1 times, most recent failure: Lost task 0.0 in stage 93.0 (TID 96) (cae9914ebdff executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/local/lib/python3.11/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/lib/python3.11/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/pyspark/rdd.py\", line 2849, in takeUpToNumLeft\n    yield next(iterator)\n          ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/pyspark/rdd.py\", line 4260, in map_values_fn\n    return kv[0], f(kv[1])\n                  ^^^^^^^^\n  File \"<ipython-input-85-271fa4f254c1>\", line 7, in calculate_average\nTypeError: 'int' object is not callable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat jdk.internal.reflect.GeneratedMethodAccessor102.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/local/lib/python3.11/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/lib/python3.11/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/pyspark/rdd.py\", line 2849, in takeUpToNumLeft\n    yield next(iterator)\n          ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/pyspark/rdd.py\", line 4260, in map_values_fn\n    return kv[0], f(kv[1])\n                  ^^^^^^^^\n  File \"<ipython-input-85-271fa4f254c1>\", line 7, in calculate_average\nTypeError: 'int' object is not callable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script will output the average grades for each student ID, maintaining the structure of the RDD with the student ID as the key.\n",
        "\n",
        "#### cogroup\n",
        " The cogroup transformation is used to group data from two or more RDDs based on their key. It returns an RDD consisting of pairs where the key is found in the original RDDs, and the value is a tuple containing Iterable collections of values for that key from each RDD."
      ],
      "metadata": {
        "id": "NZQKqrjUJ89x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create RDD with student ID and names\n",
        "student_names = sc.parallelize([(1, \"John\"), (2, \"Sally\"), (3, \"Bob\")])\n",
        "\n",
        "# Create RDD with student ID and courses\n",
        "courses = sc.parallelize([(1, \"Math\"), (2, \"History\"), (1, \"Biology\"), (3, \"Chemistry\"), (2, \"Physics\")])\n",
        "\n",
        "# CoGroup the RDDs\n",
        "cogrouped_data = student_names.cogroup(courses)\n",
        "\n",
        "# Collect and print results\n",
        "results = cogrouped_data.collect()\n",
        "\n",
        "for student_id, (names, course_list) in results:\n",
        "    print(f\"Student ID: {student_id}\")\n",
        "    print(f\"Name: {list(names)}\")\n",
        "    print(f\"Courses: {list(course_list)}\")\n",
        "    print(\"---\")"
      ],
      "metadata": {
        "id": "nffae1DQKbXO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cdc8a92-993a-4df5-bafc-3c4964a3fedc"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student ID: 2\n",
            "Name: ['Sally']\n",
            "Courses: ['History', 'Physics']\n",
            "---\n",
            "Student ID: 1\n",
            "Name: ['John']\n",
            "Courses: ['Math', 'Biology']\n",
            "---\n",
            "Student ID: 3\n",
            "Name: ['Bob']\n",
            "Courses: ['Chemistry']\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cogroup operation groups the values for each key in both RDDs into a single pair, where each value is an iterable collection."
      ],
      "metadata": {
        "id": "2lqVk3N6KiN3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### More Actions"
      ],
      "metadata": {
        "id": "VbsOVHatJfv_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####save\n",
        "Saving an RDD in PySpark can be done in a variety of formats. Common formats include saving as text files, sequence files, or other file-based data sources. Below are examples of how to save an RDD that contains customer data as a text file.\n",
        "\n",
        "But we will see about this action after the NoSQL lecture.\n",
        "\n",
        "End of Demo\n",
        "\n",
        "Thank you for the patient listening. 🙏🌞"
      ],
      "metadata": {
        "id": "hpntLVNWFmeO"
      }
    }
  ]
}