#%%
# Install pyspark and findspark
!pip install --ignore-install -q pyspark
# Install findspark library
!pip install --ignore-install -q findspark
#%%
# Import findspark
import findspark
findspark.init()
#%%
from google.colab import drive
drive.mount('/content/drive', force_remount=True)
#%%
import sys
sys.version_info
#%%
print(sys.version)
#%% md
### 1. Set up spark context and SparkSession
#%%
from pyspark.sql import SparkSession

spark = SparkSession \
    .builder \
    .appName("PySpark-LinearRegression_Advertising") \
    .getOrCreate()
#%% md
### 2.  Load data set
#%%
df = spark.read.format('com.databricks.spark.csv').\
                               options(header='true', \
                               inferschema='true').load("/content/drive/MyDrive/Advertising.csv",header=True);
#%%
df.show(6)
#%%
df.printSchema()
#%% md
### 3. Convert data into feature
#%%
from pyspark.sql import Row
from pyspark.ml.linalg import Vectors
#%%
# convert the data to dense vector
#def transData(row):
#    return Row(label=row["Sales"],
#               features=Vectors.dense([row["TV"],
#                                       row["Radio"],
#                                       row["Newspaper"]]))
def transData(data):
    return data.rdd.map(lambda r: [Vectors.dense(r[:-1]),r[-1]]).toDF(['features','label'])
#%% md
### 4. Transform the dataset to DataFrame
#%%
#transformed = df.rdd.map(transData).toDF()
data= transData(df)
data.show(6)
#%% md
### 5. Convert features data format and set up training and test data sets
#%%
from pyspark.ml import Pipeline
from pyspark.ml.regression import LinearRegression
from pyspark.ml.feature import VectorIndexer
from pyspark.ml.evaluation import RegressionEvaluator
featureIndexer = VectorIndexer(inputCol="features", \
                               outputCol="indexedFeatures",\
                               maxCategories=4).fit(data)

# Split the data into training and test sets (40% held out for testing)
(trainingData, testData) = data.randomSplit([0.6, 0.4], seed = 218)
#%% md
### 6. Fit linear regression model
#%%
# Fit elastic net model
lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)
#%%
# Chain indexer and tree in a Pipeline
pipeline = Pipeline(stages=[featureIndexer, lr])
#%%
# Train model.  This also runs the indexer.
model = pipeline.fit(trainingData)

#%%
 lrmodel= model.stages[1]
#%%
lrmodel.coefficients
#%%
lrmodel.summary.meanAbsoluteError
#%% md
###  7. Make predictions
#%%
predictions = model.transform(testData)
#%%
# Select example rows to display.
predictions.select("prediction", "label", "features").show(5)
#%% md
### 8. Evaluation
#%%
# Select (prediction, true label) and compute test error
evaluator = RegressionEvaluator(
    labelCol="label", predictionCol="prediction", metricName="rmse")
rmse = evaluator.evaluate(predictions)
print("Root Mean Squared Error (RMSE) on test data = %g" % rmse)
#%%
y_true = predictions.select("label").toPandas()
y_pred = predictions.select("prediction").toPandas()
#%%
import sklearn.metrics
r2_score = sklearn.metrics.r2_score(y_true,y_pred)
print(r2_score)
#%% md
### 9. Fit generalized linear regression model
#%%
# Import LinearRegression Class
from pyspark.ml.regression import GeneralizedLinearRegression

# Define LinearRegression Model
glr = GeneralizedLinearRegression(family="gaussian", link="identity",\
                                 maxIter = 10, regParam=0.3)
#%%
# Create pipeline
pipeline = Pipeline(stages=[featureIndexer,glr])
model = pipeline.fit(trainingData)
#%%
# Make predictions
predictions = model.transform(testData)
#%%
# Select example rows to display.
predictions.select("prediction", "label", "features").show(5)
#%%
# Evaluation
evaluator = RegressionEvaluator(
    labelCol="label", predictionCol="prediction", metricName="rmse")
rmse = evaluator.evaluate(predictions)
print("Root Mean Squared Error (RMSE) on test data = %g" % rmse)
#%%
spark.stop()
#%%
